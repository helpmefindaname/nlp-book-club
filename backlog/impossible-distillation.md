# Impossible Distillation: from Low-Quality Model to High-Quality Dataset & Model for Summarization and Paraphrasing

[Paper link](https://arxiv.org/pdf/2305.16635.pdf)

## Preliminaries

* How does Summarization and Paraphrasing work?
* What is the current state of the art in Summarization?
* What is the current state-of-the-art in Paraphrasing?
* What is the concept of knowledge distillation?
* What is the difference between soft distillation and hard distillation?
* What is Nucleus-Sampling?

## Introduction

* What are the pain points that summarization is hard?
* How does IMPOSSIBLE DISTILLATION work on a high level?
* What is the promise of this paper?

## Impossible Distillation

* What is decoding-guided distillation?
* Looking at **Figure 2** how does the constrained generation work?
* How can the samples be filtered to ensure good summarizations?
* How does the self distillation work?
* What is the setup of their distillation pipeline?

## Experiments

* What datasets are used for evaluation?
* Looking at **Table 2** how does Impossible Distillation compare to other models on a automated evaluation?
* Why is human evaluation important for summarization?
* For human evaluation what do *fluent*, *concise* and *faithful* refer to?
* What are the findings of the new generated dataset DimSum?

## Conclusion

* What do you think of Impossible Distillation?
* Do you think you could use this for potential projects?
* What references do you find most interesting?