# LM-Debugger: An Interactive Tool for Inspection and Intervention in Transformer-Based Language Models

[Paper link](https://arxiv.org/pdf/2204.12130.pdf)
[Github](https://github.com/mega002/lm-debugger)
[LM-Debugger](https://lm-debugger-l.apps.allenai.org/)

# Preliminaries

* What are the key components of Transformer?

## Introduction

* What are the 3 capabilities of LM-Debugger?
* What are the two Use cases for this tool?

## Underlying Interpretation Method
* Which components of the Transformer are used?
* How does the FFN output as weighted collection work?

## LM-Debugger

* How does the Prediction View work? Which parts of the can be seen in **figure 2** and how do they work?
* How does the Exploration view work?
* How does the Keyword Search (**figure 3**) work?
* What does the Cluster Visualization (**figure 4**) do?

## Using LM-Debugger
* Can you follow their example for tracing predictions and reproduce their findings by using their app?
* Can you follow their example of Configuring Effective Interventions for Controlled Text Generation?
* Can you follow their example of Controlling the Sentiment of Generated Text?

## Related Work
* What other work exists and how does it roughly work?

## Discussion

* What do you think of LM-Debugger?
* What possibilities do you see?
* Do you think it could affect your work live?
* Do you have ideas for concept that could be added?
* How many stars does their Github have?
* Which reference do you find most interesting?
