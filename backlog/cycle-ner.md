# CycleNER: An Unsupervised Training Approach for Named Entity Recognition

[Paper link](https://dl.acm.org/doi/pdf/10.1145/3485447.3512012)

## Introduction
* What are the common problems with NER?
* Which 2 components does CycleNER describe?

## Paper
* How does Cycle-Consistency Ner work?
* What changes does CycleNer do to ordinary Cycle-Consistency Ner?
* How does NER as Seq2Seq work?
* What is Iterative Back-Translation and how does CycleNer adopt it?
* How is data for E-cycle seps created?
* Which parameters are updated in which training step?
* How are Entity Sequences generated?

## Evaluation
* What are the baselines? Are they good or is something missing?
* What evaluation scenarios are tested?
* What is the impact of Ground truth entity sequences?
* How does the Number of Sentences impact performance?
* How does Supervised and Unsupervised compare?
* Can CycleNER be used to detect new entity types?

## Discussion
* What do you think of CycleNER? Can you imagine using it at work?
* What reference do you like the most?