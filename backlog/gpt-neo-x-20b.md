# GPT-NeoX-20B: An Open-Source Autoregressive Language Model

[Paper](https://arxiv.org/pdf/2204.06745v1.pdf)

## Introduction

* Why are Large Language Models (LLMs) important?
* What are available open language models?
* Why do the Authors think that it is important to have LLMs public available?
* What observations, that are contrary to prior believes, have been found?

## Model Design

* What are the key numbers of the model?
* What are Rotary Position Embeddings and how are they computed?
* Why are Attention and FF Layers be computed in parallel?
* What implementation mistake was made too late to be fixed?
* What is the small init scheme?
* What software was used for the training?
* What hardware was used for the training?
  
## Training
* How were the hyperparameters concluded?
* What techniques were used while training?
* How was the training data conducted?
* How transparent is the training data used?
* What design choices were made for tokenization?
* What is the common conclusion of duplicates in training data? Did this training agree with those findings?

## Performance Evaluation
* What benchmark is taken for evaluation?
* Which models are chosen for comparison? Which models were excluded for which reason?
* What tasks are evaluated? 

## Discussion
* What are the findings towards zero-shot-learning vs 5-shot-learning?
* What are the findings about mathematics?

## Limitations
* What limitations do the Authors set?
* What are their considerations for releasing a 20B LLM? What is your opinion about that?

## Final thoughts
* What is your opinion on GPT-NeoX?
* Have you managed to try it out (for example [here](https://textsynth.com/playground.html))?
* What references did you find interesting?


